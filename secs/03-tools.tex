% !TEX root = ../swarmOptimTutorial.tex


\section{Optimal control and optimization tools}\label{sec:3}

Introduce optimization types and seminal/book references

\subsection{Optimal control and dynamic programming}

\subsubsection{Optimal Control}
Consider a controlled dynamical system: $\dot{x}=f(x,u)$, $x(0)=x^0$. Then optimal control corresponds to finding a control $u^*$ which minimizes the cost functional
$$
J[u(.)]:=\int_0^T r\left(x(t), u(t) \right) dt + g(x(T)),
$$
where $r(.,.)$ is the running cost, $g(.)$ the terminal cost, and $T>0$ is the terminal time. Typically, the minimization is performed under state space and control constraints: $x(t) \in \mc X$ and $u(t) \in \mc U$ for all $t \in [0,T]$. 

In the discrete time and infinite horizon formulation, the dynamic programming formulation is given by 
\begin{align*}
\text{minimize} & \quad J = \sum_{t=0}^{\infty} r(x(t),u(t)) \\
\text{subject to} & \quad u(t) \in \mc U, \, x(t) \in \mc X, \quad t = 0, 1, \ldots \\
& \quad x(t+1) = A x(t) + B u(t), \quad t = 0, 1, \ldots \\
& \quad x(0) = x^0
\end{align*}

The dynamic programming approach involves solving for a value function $V(x^0)$ as a function of the initial state $x^0$ that satisfies the Bellman or dynamic programming equation:
\e
\begin{array}{rl}
V(x^0)=\inf & \{ r (x^0,w) + V(Ax^0+B w), w \in \mc U, \\
		& \,\, Ax^0 + Bw \in \mc Z  \}.
		\nonumber
 \end{array}
\ee

The optimal control $u$ at time $t$ is given by 
$$
u^*(t) = \underset{w \in \mc U, \, A x(t) + B w \in \mc X} {\argmin} \left(r(x(t),w) + V(A x(t) + Bw) \right)
$$

\subsubsection{Model Predictive Control}
At each time $t$, one solves the following problem:
\begin{align*}
\text{minimize} & \quad \sum_{\tau=t}^{t+T} r(x(\tau),u(\tau)) \\
\text{subject to} & \quad u(\tau) \in \mc U, \, x(\tau) \in \mc X, \, \, \tau = t, \ldots, t+ T \\
& \quad x(\tau +1) = A x(\tau) + B u(\tau), \, \, \tau = t, \ldots, t+ T \\
& \quad x(t+T)= 0
\end{align*}
If $\tilde{x}(t+1), \ldots, \tilde{x}(t+T), \tilde{u}(t), \ldots, \tilde{u}(t+T-1)$ is the optimal plan of action, then the optimal control at time $t$ is given by $u^*(t)=\tilde{u}(t)$. The above minimization problem will be resolved at time $t+1$, which will then yield the optimal control for time $t+1$.
More details can be obtained in
\citet{Maciejowski:2002wc, Camacho:2004tg, Borrelli:2011uw}.

\subsection{Constrained optimization}

The goal of an optimization problem is to compute the values of a set of variables $\x \in \X$ which minimize an objective function subject to a set of inequality and equality constraints.
\e
\begin{array}{rlll}
\x^* := & \arg \underset{\x}{\min} & f(\x) 		& \\
 	& \tn{subject to} 		& g_i(\x) \leq 0 	& \forall i \in \{1,\dots, n_{ineq} \} \\
  	&	  		  		& h_i(\x) = 0 	& \forall i \in \{1,\dots, n_{eq} \}
 \end{array}
\ee

Optimization problems are classified depending on the type of variables and constraints. We name some of the most common ones. For further reading, \citet{Nocedal:2006uv} provides a good overview on numerical optimization, \citet{Bertsekas:1999ua} in non-linear programming and \citet{Boyd:2004uz} in convex optimization. A large number of state of the art solvers exist, popular ones include OOQP, CPLEX, GUROBI, MOSEK and CVXGEN.

\subsubsection{Convex optimization with continuous variables $\x \in \R$}
This family of problems can be solved extremely efficiently.
These can be further classified into \emph{Linear programming LP} if the cost and constraints are linear,
\emph{Quadratic programming QP} if the cost and constraints are quadratic and convex and \emph{Semi-Definite Programming SDP} if a linear objective function is minimized over the intersection of the cone of positive semidefinite matrices with an affine space.

\subsubsection{Non-convex optimization with continuous variables}
General non-linear problems, where $\x \in \R$, that can not be formulated as a QP or SDP can be solved via \emph{Gradient-based Methods} or \emph{Sequential Convex Programming SCP} where a local solution is found by iteratively linearizing the problem. A global solution can be found via heuristic or probabilistic \emph{search techniques}.  

\subsubsection{Optimization with integer variables}
Consider the case where some of the variables $\x_j$ take integer, $\x_j \in \N$, or typically binary, $\x_j \in \{0,1\}$, values.
%For integer programs with only binary variables, a solution can be found via explicit enumeration.
This defines a \emph{Mixed Integer Progam MIP}, namely MILP or MIQP depending on if the cost and constraints are linear or quadratic. This problem is inherently non convex and can be solved via branch-and-bound with search heuristics, where several LPs/QPs are solved for changing values of the binary variables. A special case are Integer Linear Programs (ILP) which can be formulated as a \emph{network flow} problem and by solving an equivalent LP the optimal integer solution is found,

\subsection{Combinatorial optimization}

Combinatorial optimization consists of finding an optimal object from a finite set of objects~\cite{Ahuja:1993tk}. In many such problems, exhaustive search is not feasible due to the large cardinality of the set and suboptimal algorithms must be devised. 
A canonical one is the Greedy algorithm, which performs well on some optimization problems, but can do poorly on others.

Some common problems involving combinatorial optimization are the traveling salesman problem ("TSP") and the minimum spanning tree problem ("MST").

